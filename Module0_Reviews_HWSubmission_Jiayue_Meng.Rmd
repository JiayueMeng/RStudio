---
title: "Module 0 Assignment on Tests"
author: "Jiayue Meng // Undergraduate"
date: "2/9/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***


***

\newpage{}

## Instruction

In Module 0, we briefly reviewed seven **statistical tests methods** in data analysis:

  1. Conventional (z- or t-) 
  2. Permutation (randomization-based, all permutations)
  3. Randomization or Simulated Permutation (simulation-based, some permutations, without replacement)
  4. Bootstrapping (resampling-based, with replacement)
  5. Linear Regression (theory-based, LS-based)
  6. and 7. Nonparametric Approaches: Wilcoxon and Rank-based (median test would be used as well)

In the assignment, you will run these tests with a new data set for two-sample independent mean problem below.

An experiment was administered whether or not extra nitrogen affects the stem weight on seedlings. One group (control, n=8) was controlled with **standard nitrogen**, the other group (treatment, n=8) was given **extra nitrogen**. After two weeks, the stem weights were measured in gr. Assume the populations of the samples are normally distributed with unknown equal variances. 

The raw data is as follows:

```{r echo=TRUE, results='hold'}
control_group <- c(.40, .45, .35, .27, .46, .33, .30, .43)
trtmnt_group <- c(.49, .45, .35, .38, .48, .55, .47, .65)
boxplot(control_group, trtmnt_group)
round(sd(control_group),2)
round(sd(trtmnt_group),2)
```

```{r echo=TRUE}
# Or, use this way, or make data frame so you can run directly the Module0 R Codes
datam = cbind(c(control_group, trtmnt_group), c(rep(0, 8), rep(1, 8)))
y <- datam[,1]
group <- datam[,2]
colnames(datam)=c("y","group")
#View(datam) #Check the data
```


## Module Assignment Questions

1) (*Descriptive*) Do descriptive analysis.

a. Is the study design an experimental or observational study? Justify. 
b. Obtain a side-by-side boxplot on measurements of the two groups. Include the graph. Add title and group names. 
c. Obtain summary statistics that show central (mean, median etc) and spread (sd, IQR, range etc) measurements of the data distribution for each group. Make a table and include the statistics. 
d. Compare the centers and spreads. Do you see a difference in centers? Do you see a difference in spreads? Comment.
e. Do you see any potential outliers? Find and comment if exists. Explain your criterion to find outliers.


***

2) (*Test*) Using the seven methods above, test at a 5% significance level if the difference in the mean stem weight between seedlings that receive regular nitrogen and those that receive extra nitrogen is not equal (test if there is no difference between the two means on treatment and control groups). 

a. Write the hypotheses.
b. What is a hypothesis testing? Write what you know about hot to test in general.
c. Run the seven tests and make a comparative table, showing the `p-values`. (You need to make the table showing all p-values. Modify the lab codes for this data set. Make sure you run line by line)
d. Conclude each result with a short comment at the specified significance level (answer part c and d together, put all the p-value calculations and the comments in a table)
e. Now, write an `overall comment` on the results that communicate with the goal of the problem. Use the context.


***

3) (*Concepts*) Analyze the validity of some tests.

a. List all the assumptions on `t-test`. 
b. Do the assumptions meet? Check each.
c. List all the assumptions on `Wilcoxon test`. Do the assumptions meet?
d. Do you know the assumptions on `linear regression` with LS? (a simple answer works: yes/no. if you know, write all. if not, it is fine we will learn)
e. The three methods are based on randomization or resampling. What are the merits of doing randomization or resampling? Which of the three methods would work well for large data situation? Why?


***

4) (*Extension*) Deep analysis and pitfalls.

a. List the `type of errors` (either Type I or II) committed in the decision made for each test. Make a table that shows all. Describe what Type I and Type II error rates are.
b. Build a 95% `confidence interval` on mean difference between treatment and control groups using t-critical value from df=n1+n2-2 and t-test's standard error formula. Interpret what it says. Does it confirm the p-value result from t-test? 
c. Obtain a `percentile confidence interval` (2.5th to 97.5th) on mean difference for the permutation test (using the permutation sampling distribution of differences on mean). Include the confidence interval. Interpret.

Note: In the `(exact) Permutation test` we do in the slides, take account all the combination/permutation groups (n1+n2 choose n1 or C(n1+n2, n1)) of the combined data (n1+n2) by splitting into two groups of size n1 and n2. Then, each permuted groups pair (two groups) should yield the data, therefore, you can calculate the difference in mean. The code I provided in the r lab does this simulation. Once you obtain the distribution of these differences in mean, this will be called 'sampling dist of difference in mean' of the permutation method so you will use it to test the hypothesis and calculate the confidence interval on $\mu_1-\mu_2$.

d. Compare the confidence intervals calculated in part b and c. Which one is more precise? Which method is more efficient? Comment.
e. Corrupt the data value .40 as 40 in the control group - as if you make a typo so this is a `bad outlier`. Recalculate the p-values of all tests. What changed? Which tests didn't change dramatically?
f. (BONUS) `Standard error` is basically defined as the standard deviation of the sampling distribution of differences in mean. Either use the R packs or use the std of test statistics simulated for data. Can you find the standard error on mean difference estimate for each test? Do your best to find each. Which one(s) are most efficient test(s)? Why?
g. (BONUS) Ask a challenging question and answer (under the assignment context).

\newpage

***


## Your Solutions

Q1) 

Part a: The study design an experimental study because it has a control group and a treatment group. Researchers introduce an intervention and study the effects.


***
Part b:
```{r}
y<-c(.40, .45, .35, .27, .46, .33, .30, .43, .49, .45, .35, .38, .48, .55, .47, .65)
group <-rep(0:1,each=8)
datam<-cbind(y, group)
colnames(datam)=c("control_group","trtmnt_group")

boxplot(y~group, data=datam, main="Nitrogen: Control (0) and Treatment (1) groups", ylab="Stem Weights(gr.)", xlab="Group")
```


***
Part c
```{r}
summ=matrix(c(16, 8, 8, mean(y), mean(y[group==0]), 
              mean(y[group==1]),sd(y), sd(y[group==0]), 
              sd(y[group==1]), median(y), median(y[group==0]),
              median(y[group==1]),IQR(y), IQR(y[group==0]), 
              IQR(y[group==1]), range(y), range(y[group==0]),
            range(y[group==1])), 3, 6)

colnames(summ)=c("Size (n)", "Mean","SD","Median","IQR","Range")
rownames(summ)=c("All","Control","Treatment")

table1=round(summ,2)

table1

# Lets use a better format from kable
knitr::kable(table1, caption = "Summary Statistics on Sten Weight Data")

```


***
Part d:
```{r}
library(lattice)
stripchart(y~group, method="stack", pch=2:4, 
           col=c("blue", "red"), cex=1, 
           main="Nitrogen: Control (0) and Treatment (1) groups", 
           pty='m', ylab = "Group", xlab="Stem Weights(gr.)")

```
Center: The mean of the control group is 0.37	and the median is 0.38. The mean of the treatment group is 0.48 and the median is 0.48. We can see that the treatment group has a higher mean and a higher median. The center of the treatment group is higher than that of the control group. 


Spread: The standard deviation of the control group is 0.07. The standard deviation of the treatment group is 0.09. Thus, the treatment group has a larger standard deviation. The IQR of the control group is	0.11 and the range is	0.65. The IQR of the treatment group is	0.07 and the range is	0.27. So, the treatment group has a smaller IQR and a smaller range. The spread of the treatment group is larger than the spread of the control group.


Shape: There is one outlier 0.65 in the treatment groups. 


***
Part e: 


```{r}
summary(control_group)
summary(trtmnt_group)
```


Control Group: Lower bound = Q1-1.5 IQR = 0.3225-1.5*0.11 = 0.1575. 
               Upper bound = Q3+1.5 IQR = 0.4350+1.5*0.11 = 0.6.
               There is no point larger than 0.6 or smaller than 0.1575, so there is no potential outlier in the control group.


Treatment Group: Lower bound = Q1-1.5 IQR = 0.4325-1.5*0.07 = 0.3275. 
                 Upper bound = Q3+1.5 IQR = 0.5050+1.5*0.07 = 0.61.
                 There is one outlier 0.65 that is larger than 0.61, and there is no point smaller than 0.3275, so there is one potential outlier in the treatment group.


***



\newpage

Q2) 

Part a


Let $\mu_{control}$ be the mean stem weight with standard nitrogen, and $\mu_{treat}$ be the mean stem weight with extra nitrogen.


$H_0$: $\mu_{control}$ = $\mu_{treat}$
$H_0$: $\mu_{control}$ $\neq$ $\mu_{treat}$


***
Part b:


Hypothesis testing is a procedure based on sample evidence and probability used to test events regarding population parameters. We begin hypothesis testing by claiming the population parameter (typically the mean). Null hypothesis $H_0$ is that of no change or status quo. The alternative hypothesis $H_1$ is a second statement that contradicts $H_0$. Either $H_0$ or $H_1$ is true. We believe the null hypothesis to be true unless overwhelming evidence exist to the contrary. Once we formulate our hypotheses, we need to draw random samples from the population of interest. 


How to test in general: 
1. Check the conditions required for the validity of the test.
2. Define the parameter of interest in the context of the problem.
3. State the null hypothesis.
4. State the alternative hypothesis.
5. Determine the proper test to use, and calculate the test statistic.
6. Calculate the p-value or critical value.
7. Make "reject/fail to reject" decision.
8. State the conclusion in the context of the problem.



***
Part c:

1. Difference and T-test: the p-value is 0.0262.
```{r}
y <- c(.40, .45, .35, .27, .46, .33, .30, .43, .49, .45, .35, .38, .48, .55, .47, .65)
group <-rep(0:1 ,each=8)

## 1. Difference and T-test
# difference in means
diff=mean(y[group==1])-mean(y[group==0])
diff
cat(diff, 'is the difference in means (observed statistic); ', '\n')

# 2-maple independent t-test, assuming normality and equal variances
t2w=t.test(y~group, var.equal = TRUE) 
summary(t2w)
str(t2w)
t2w$stderr

# standard error and test statistic using sample sd's
t2se = sqrt(0.07^2/8+0.09^2/8)
ts= diff/t2se
cat(t2se, 'standard error; ')

cat(ts, 'is test statistic; ') #

# p-value of the test
t2p=t2w$p.value
cat(t2p, 'is the p-value of the test;')

# let's combine all
D=as.matrix(t(c(diff, t2se, ts, t2p)))
colnames(D)=c("mean difference", "se", "ts", "p-value")
round(D,4)

# Lets use a better format from kable
knitr::kable(round(D, 4), caption = "T-test results")

```


2. Permutation Test - exact method using long way: 0.0266 is the exact p-value.
```{r}
## 2. Permutation Test - exact method using long way
library(PASWR) # using allocation functions
set.seed(99)
help(SRS) #try 
length(y)
alloc=SRS(y,8) # see alloc[1,]
dim(alloc) #factorial(n)/(factorial(n-r)*factorial(r))
alloc[1,] #see an sample result

# how many all combinations
reps=choose(16,8) #sample sizes vary
Comb <- matrix(rep(y,reps),ncol=16, byrow=TRUE)
pdT6<-SRS(1:16,8)
Theta=array(0,reps)
for(i in 1:reps){
 Theta[i]=mean(Comb[i,pdT6[i,]])-mean(Comb[i,-pdT6[i,]])
}

# p-value calculation
num_out= sum(Theta>=diff | Theta<=-diff) #how many are out of threshold
pep=num_out/reps
cat(reps,',', num_out, ': number of all combinations and number beyond threshold\n')
cat(round(pep,4), 'is the exact p-value.')

```


3. Randomization Test or Approximated Perm test: 0.0278 is the p-value.
```{r}
## 3. Randomization Test or Approximated Perm test
# Simulate and resample w/o replacement
reps2 <- 10000 #B=10000
results <- numeric(reps2)
set.seed(99)
temp<-sample(y)
for (i in 1:reps2) { 
  temp <- sample(y, replace = FALSE) 
  results[i] <- mean(temp[1:8])-mean(temp[9:16]) 
} 

# p-value
psp=sum(results>=diff | results<=-diff)/(reps2+1)
cat(round(psp, 4), 'is the p-value.') 
```

4. Bootstrapping: 0.0271 is the p-value.
```{r}
## 4. Bootstrapping
# Bootstrapping (w/ replacement)
reps2 <- 10000 #B=10000
results2 <- numeric(reps2)
set.seed(99)
for (i in 1:reps2) { 
temp <- sample(y, replace=TRUE) 
results2[i] <- mean(temp[1:8])-mean(temp[9:16]) 
} 
bp=sum(results2>=diff | results2<=-diff)/(reps2+1) 
cat(round(bp, 4), 'is the p-value.')
```


5. Simple Linear Regression: slope estimate is 0.1038 and p-value is 0.0262.
```{r}
## Simple Linear Regression

## Lin Reg
# ls linear model y~group: why is this?
plot(group, y, xlab="0-Control Group, 1-Treatment Group",main="Regression approach")

# fit a linear model with lm(y~..)
lw=summary(lm(y~group))
abline(lw)

le=lw$coefficients[2]
lp=coef(lw)[2,4]

cat(round(c(le, lp),4), ': slope estimate and p-value')
```


```{r}
## 6-7. Nonparametric
# Basically: one of the three methods are good to know:
# Wilcoxon, Rank-based, and Median test
# Now: Wilcoxon test for two-sample independent problem
g1=y[group==1]
g0=y[group==0]
fitw=wilcox.test(g0,g1, exact=FALSE)
wp=fitw$p.value
cat(wp, ': wilcoxon p-value')

# need packs for nonparametric calculations
library(quantreg)
library(Rfit) #rank-based

# rank-based test
# read the pack from help(rfit)
fit.r=summary(rfit(y~group))
#str(fit.r)
fit.r$coefficients
mp=fit.r$coefficients[2,4]
cat(mp, ': rank-based p-value')

# median-based test
fit.md=summary(rq(y~group,.5)) 
str(fit.md) # check what stored/calculated
fit.md$coefficients
md_table=fit.md$coefficients

## Results table
C=round(as.matrix(c(t2p,pep,psp,bp,lp,wp,mp)),4)
colnames(C)=c("P-value")
rownames(C)=c("t-test","perm","random","bootst","linear reg","wilcoxon", "rank-based")

# Lets use a better format from kable
knitr::kable(C, caption = "Seven Methods Results")

```



***
Part c and d:
```{r}
## Results table
A=round(as.matrix(c(t2p,pep,psp,bp,lp,wp,mp)),4)
ftr="Fail to reject the null"
rn="Reject the null in favor of the alt"
B=c(rn,rn,rn,rn,rn,rn,ftr)
C=cbind(A,B)
colnames(C)=c("P-value","Decision at 5% level")
rownames(C)=c("t-test","perm","random","bootst","linear reg","wilcoxon", "rank-based")
C

# Lets use a better format from kable
knitr::kable(C, caption = "Seven Methods Results")
```


***
Part e:


According to the result table above, we can see that all tests except the rank-based test reject $H_0$. Therefore, we have sufficient evidence to conclude that, for all the seven tests except the rank-based test, extra nitrogen affects the stem weight on seedlings. 

***

\newpage


Q3) 

Part a:


Two-Sample T-Test Assumptions:

1. Independence of the observations. Each subject should belong to only one group. The observations in groups are not related.
2. Normality. The data for each group should be approximately normally distributed.
3. Homogeneity of variances. The variance of the outcome variable should be equal in each group. However, if the t-test is the Welch t-test, we do not make this assumptions.
4. Simple random sample. Both samples are simple random samples from their respective populations. Each individual in the population has an equal probability of being selected in the sample.
5. The data are continuous (not discrete) or ordinal.


***
Part b:


All assumptions are met. From the question, we know that observations are independent and variances are equal. The data are continuous because stem weight is a continuous variable. Simple random sample assumption is also met because we used SRS.



***
Part c:


1. Dependent samples. The two samples need to be dependent observations of the cases. 

2. Independence. The paired observations are selected randomly and independently.

3. Continuous dependent variable â€“ Although the Wilcoxon signed rank test ranks the differences according to their size and is therefore a non-parametric test, it assumes that the measurements are continuous in theoretical nature.  To account for the fact that in most cases the dependent variable is binominal distributed, a continuity correction is applied.

4. Ordinal level of measurement. Both dependent measurements to be at least of ordinal scale.

5. The observations must be comparable. For every difference of observations, it must be clear which one is greater of if both observations are equal.

6. The distribution of each difference is symmetric.

All assumptions are met.


***
Part d: No.

***
Part e:


The three methods that are based on randomization: randomization test, permutation test, and bootstrapping. 


Resampling and Randomization can improve the accuracy and quantify the uncertainty of a population parameter. They can also reduce selection bias. Since individuals are chosen at random, each individual in the large population set has the same probability of being selected. Thus, the subset can represents the larger population better. 


T-test will work well for large data situation. Linear regression will not work well for large data situation because it requires a linear relationship between the two samples. Likewise, wilcoxon test can only solve ranks of sample values.


***

\newpage


Q4) 

Part a:


Type I error rate: the rate for rejection of a true null hypothesis.
Type II error rate: the rate for the failure of rejection of a false null hypothesis.
```{r}
A1=round(as.matrix(c(t2p,pep,psp,bp,lp,wp,mp)),4)
A2=c(0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0)
A3=c(0,0,0,0,0,0,mp)
tabl=cbind(A1,A2,A3)
colnames(tabl)=c("P-value","Type I error rate", "Type II error rate")
rownames(tabl)=c("t-test","perm","random","bootst","linear reg","wilcoxon", "rank-based")
# Lets use a better format from table
knitr::kable(tabl, caption = "Seven Methods Results of Type of error")
```



***
Part b:
```{r}
left_bound<-diff-qt(1-0.05/2, 14)*t2se
right_bound<-diff+qt(1-0.05/2, 14)*t2se
cat("The 95% confidence interval on mean difference between treatment and control groups is (",left_bound,", ", right_bound,").")

```


We are 95% confidence that the true mean difference between treatment and control groups falls in the interval ( 0.01729088 ,  0.1902091 ). It confirmS the p-value result from t-test because the t statistic 2.573721 (we know from Question2 part b) is not in the interval ( 0.01729088 ,  0.1902091 ). 


***
Part c:
```{r}
Theta <- sort(Theta,decreasing = FALSE)
quantile(Theta,c(0.025,0.975))
```


(-0.09375, 0.09375) is the confidence interval on mean difference for permutation test. We are 95% confident that the interval (-0.09375, 0.09375) contains the true mean difference between the control group and treatment group.


***
Part d:


The confidence interval calculated in part b is more precise than the confidence interval calculated in part c because C.I. in part b has a smaller range. The method in part b is also more efficient than the method in part c because it has smaller standard error and needs a lot of calculation.


***
Part e:


1. Difference and T-test: the p-value is 0.3445.
```{r}
y <- c(40, .45, .35, .27, .46, .33, .30, .43, .49, .45, .35, .38, .48, .55, .47, .65)
group <-rep(0:1 ,each=8)

## 1. Difference and T-test
# difference in means
diff=mean(y[group==1])-mean(y[group==0])
diff
cat(diff, 'is the difference in means (observed statistic); ', '\n')

# 2-maple independent t-test, assuming normality and equal variances
t2w=t.test(y~group, var.equal = TRUE) 
summary(t2w)
str(t2w)
t2w$stderr

# standard error and test statistic using sample sd's
t2se = sqrt(0.07^2/8+0.09^2/8)
ts= diff/t2se
cat(t2se, 'standard error; ')

cat(ts, 'is test statistic; ') #

# p-value of the test
t2p=t2w$p.value
cat(t2p, 'is the p-value of the test;')

# let's combine all
D=as.matrix(t(c(diff, t2se, ts, t2p)))
colnames(D)=c("mean difference", "se", "ts", "p-value")
round(D,4)

# Lets use a better format from kable
knitr::kable(round(D, 4), caption = "T-test results")

```


2. Permutation Test - exact method using long way: 1 is the exact p-value.
```{r}
## 2. Permutation Test - exact method using long way
library(PASWR) # using allocation functions
set.seed(99)
help(SRS) #try 
length(y)
alloc=SRS(y,8) # see alloc[1,]
dim(alloc) #factorial(n)/(factorial(n-r)*factorial(r))
alloc[1,] #see an sample result

# how many all combinations
reps=choose(16,8) #sample sizes vary
Comb <- matrix(rep(y,reps),ncol=16, byrow=TRUE)
pdT6<-SRS(1:16,8)
Theta=array(0,reps)
for(i in 1:reps){
 Theta[i]=mean(Comb[i,pdT6[i,]])-mean(Comb[i,-pdT6[i,]])
}

# p-value calculation
num_out= sum(Theta>=diff | Theta<=-diff) #how many are out of threshold
pep=num_out/reps
cat(reps,',', num_out, ': number of all combinations and number beyond threshold\n')
cat(round(pep,4), 'is the exact p-value.')

```


3. Randomization Test or Approximated Perm test: 0.9999 is the p-value.
```{r}
## 3. Randomization Test or Approximated Perm test
# Simulate and resample w/o replacement
reps2 <- 10000 #B=10000
results <- numeric(reps2)
set.seed(99)
temp<-sample(y)
for (i in 1:reps2) { 
  temp <- sample(y, replace = FALSE) 
  results[i] <- mean(temp[1:8])-mean(temp[9:16]) 
} 

# p-value
psp=sum(results>=diff | results<=-diff)/(reps2+1)
cat(round(psp, 4), 'is the p-value.') 
```

4. Bootstrapping: 0.9999 is the p-value.
```{r}
## 4. Bootstrapping
# Bootstrapping (w/ replacement)
reps2 <- 10000 #B=10000
results2 <- numeric(reps2)
set.seed(99)
for (i in 1:reps2) { 
temp <- sample(y, replace=TRUE) 
results2[i] <- mean(temp[1:8])-mean(temp[9:16]) 
} 
bp=sum(results2>=diff | results2<=-diff)/(reps2+1) 
cat(round(bp, 4), 'is the p-value.')
```


5. Simple Linear Regression: slope estimate is -4.8462 and p-value is 0.3445.
```{r}
## Simple Linear Regression

## Lin Reg
# ls linear model y~group: why is this?
plot(group, y, xlab="0-Control Group, 1-Treatment Group",main="Regression approach")

# fit a linear model with lm(y~..)
lw=summary(lm(y~group))
abline(lw)

le=lw$coefficients[2]
lp=coef(lw)[2,4]

cat(round(c(le, lp),4), ': slope estimate and p-value')
```

6-7.
Wilcoxon Test: 0.1031 is the p-value.
Rank-based:	0.2465 is the p-value.
```{r}
## 6-7. Nonparametric
# Basically: one of the three methods are good to know:
# Wilcoxon, Rank-based, and Median test
# Now: Wilcoxon test for two-sample independent problem
g1=y[group==1]
g0=y[group==0]
fitw=wilcox.test(g0,g1, exact=FALSE)
wp=fitw$p.value
cat(wp, ': wilcoxon p-value')

# need packs for nonparametric calculations
library(quantreg)
library(Rfit) #rank-based

# rank-based test
# read the pack from help(rfit)
fit.r=summary(rfit(y~group))
#str(fit.r)
fit.r$coefficients
mp=fit.r$coefficients[2,4]
cat(mp, ': rank-based p-value')

# median-based test
fit.md=summary(rq(y~group,.5)) 
str(fit.md) # check what stored/calculated
fit.md$coefficients
md_table=fit.md$coefficients

## Results table
C=round(as.matrix(c(t2p,pep,psp,bp,lp,wp,mp)),4)
colnames(C)=c("P-value")
rownames(C)=c("t-test","perm","random","bootst","linear reg","wilcoxon", "rank-based")

# Lets use a better format from kable
knitr::kable(C, caption = "Seven Methods Results")

```


The p-values of all tests became larger. Comparing to other tests, Rank-based test and Wilcoxon test did not changed dramatically.


***
Part g:


Question: Please calculate the margin of error for Question 4 part b.
Answer: 
```{r}
m=(0.1896715-0.01782852)/2
m
```
The margin of error for Question 4 part b is 0.08592149.


***

\newpage

### Write comments, questions: ...


***
I hereby write and submit my solutions without violating the academic honesty and integrity. If not so, I accept the consequences. 

### List the fiends you worked with (name, last name): Xubin Lou

### Disclose the resourcrs or persons if you get any help: 
Online resources. 

***
## References

https://www.investopedia.com/ask/answers/073115/what-assumptions-are-made-when-conducting-ttest.asp
https://machinelearningmastery.com/statistical-sampling-and-resampling/
http://www.csun.edu/~an73773/Quiz3Feedback12pm.pdf
